Dataserve Documentation
Introduction
Dataserve is a cloud-based tool for managing and analyzing data. It exposes data as an API, acting as an interface between the user and the server. It has three key elements: Onboarding, Loading, and Serving. Among these, Serving is the most used element, whereas Onboarding and Loading have minimal usage.
Dataserve provides two applications for querying data: Play_id and Playbuilder_id. Play_id gives a single prediction based on location number and operating date, whereas Playbuilder_id returns multiple predictions based on store details (store number, start time, and end time). Playbuilder_id is essentially a collection of data from an organization.
Dataserve uses tokens for identification, authentication, and authorization. The Bearer token is specifically used for URL authorization. A sample URL for identifying Dataserve is:
https://aireserve-prod.azure-api.net/serve/dat/playbuilder/prod/play
Dataserve production monitoring should occur every 4 hours.
Onboarding API
The Onboarding API outlines the process of loading data into the server for processing. It is a one-time activity. The Manifest file is used to ensure the correctness of data onboarded. This file is a JSON document, which contains delta or parquet formatted data. It can be identified based on type (data or model), app name (play builder), and dataset name (play or guidance).
Loading (or Data Changer) API
The Loading API allows the ROA and AI Reserve teams to load data into Cosmos DB. Data bricks jobs are triggered once the data is loaded.
Serving API
The Serving API is a user-facing application that fetches data from Cosmos DB when a user request is triggered. It can be accessed using the Dataserve URL, bearer token, filters, subscription key, and correlation ID. Correlation ID is used to fetch unique requests from users. Both play_id and playbuilder_id are used to query data.
End-to-End Flow for Dataserve
The data onboarding process starts with the ROA & AI Reserve teams submitting the manifest, which triggers the Onboarding API and assigns a job name. This job name is invoked by the publisher scoring pipeline via the Loading API. Finally, the consumer uses play_id or playbuilder_id to query the data via the Serving API.
Dataserve Dashboard
Dataserve monitoring is done through DataDog, with several key metrics tracked:
1. Response Code-Based Metrics: Cumulative charts for 200, 400, and 500 codes.
2. Name-Based Metrics: Includes charts for codes like 200, 401, 503, 210 (No documents fetched), etc.
3. Count-Based Metrics: Displays count for the aforementioned response codes.
4. Split-Based Metrics: Includes average response times, counts for single/multiple prediction requests, percentile response times, and region-based response times.
5. Kubernetes Metrics: Includes charts for CPU and memory usage by pod and pod status.
Dataserve Basic Commands
Windows PowerShell commands for Dataserve operations include:
1. kubectl get pods -n data-as-api: Fetch details of running pods in Dataserve, including status, restarts, and pod age.
2. kubectl top pods -n data-as-api: Fetch CPU and memory usage of pods in Dataserve.
3. kubectl get hpa -n data-as-api: Fetch details like targets, min/max pods, replicas, and pod age.
4. kubectl describe pod -n data-as-api: Fetch detailed information about a specific pod in Dataserve.
RCA for Pod Restart in Dataserve
If a pod restart occurs in Dataserve, a Root Cause Analysis (RCA) is required. The steps to follow are:
1. Issue: Provide detailed information such as the region, pod name, node name, and namespace.
2. Analysis: Analyze the pods, top pods, hpa, CPU usage, memory usage, exit codes, and pod events.
