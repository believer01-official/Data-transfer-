Here's a detailed explanation for each of the **main data pipelines** in the **Digital Drive-Thru (DDT) application**:  

---

### **1. Splunk Pipeline**  
The **Splunk pipeline** is responsible for collecting and processing real-time event logs and operational data from the system. It ensures that missing data is backfilled efficiently.  

#### **Splunk_Hourly Pipeline**  
- Runs **every hour** to process event logs.  
- If an hourly run **fails**, the system waits for the next **two consecutive runs** to check if it automatically backfills the failed data.  
- If the data is **not backfilled automatically**, a **manual backfill** is performed using `splunk_adf_main_backfill`.  
- The backfill process retrieves **missing logs** based on **date and time** from an **Azure Databricks notebook**.  

#### **Splunk_Weekly Pipeline**  
- Runs **once a week** to backfill any missing data that was left out by the hourly runs.  
- Ensures that historical logs are complete for **analytics and monitoring**.  

---

### **2. Tardigrade Pipeline**  
The **Tardigrade pipeline** is a feature engineering pipeline that processes and extracts key data attributes.  

- Ingests data from **New Relic** and **Splunk**.  
- Processes data in the **tardigrade_main_master_split** pipeline.  
- Generates **product, store, weather, and hourly weather** features.  
- Stores the extracted features in **Redis cache** using the **upload_to_redis** pipeline.  
- **Xpbuff activities** in the `feature_tardigrade` pipeline send these features for **model training**.  

---

### **3. Weekly Measurement Pipeline**  
- Runs **weekly** to generate data for the **immediate previous Sunday**.  
- Extracts important data such as **variants and pricing**.  
- The processed data is **uploaded to Redis cache** for further use in recommendations.  

---

### **4. Weekly Product Insights Pipeline**  
- Generates **weekly insights** based on the latest data.  
- Helps in creating **dashboards and analytics reports**.  
- Used for tracking **product trends, customer behavior, and store performance**.  

---

These pipelines **ensure smooth data processing, feature generation, and model training** for the DDT application. Let me know if you need more details!
