The PowerPoint presentation includes the following key sections based on the information you provided:

### **1. Introduction**
- **Title:** DDT (Digital Drive-Thru) – Refresher Session  
- **Overview:** DDT provides personalized recommendations based on store location, weather, time, and customer cart activity using reinforcement learning.  

### **2. Application Overview**
- **Reinforcement Algorithm:** Uses historical product data for recommendations.  
- **Zero Cart vs. Plus Cart:**  
  - *Zero Cart:* No items added → recommendations based on store’s historical data.  
  - *Plus Cart:* Items added → personalized recommendations.  
- **Key Feature:** Real-time processing with low latency.  
- **API:** DDT application uses ModelServe.  

### **3. Application Architecture**
- Customer request enters the **store network**.  
- Passes through **security layers**:  
  - **Blue Coat Proxy** (Web security)  
  - **Retail Network Firewall** (Barrier between users & Starbucks network)  
  - **Open API** (Protection from open internet attacks)  
- Request authentication via **OAuth** → **Orchestrator** → **Deepbrew DDT Tardigrade**.  
- Processed in DDT models → Recommendations sent back.  
- **Key Metric:** Recommendation request time.  

### **4. High-Level Architecture**
- Request flow:  
  **Store Network → Open API → DDT App Services → Redis Cache → Response to Customer**  
- DDT App Services uses **three models**:  
  - **DDQN:** Categorizes request.  
  - **IMS:** Checks stock availability.  
  - **Bandits:** Generates recommendations.  
- **Redis Cache** replaces SQL DB, stores data in **ADLS Path**.  

### **5. DDT Application Flow**
- **Request Flow:**  
  1. **Orchestrator URL** → Decides request routing.  
  2. **Traffic Manager URL** → Splits traffic (CUS, EUS2).  
  3. **APIM** → API call to **Frontend Kubernetes Service**.  
  4. **Frontend Service:** Authorization → Preprocessing → Switching.  
  5. **Backend Kubernetes Service:** Uses **DDQN & DICF models**.  
  6. Processed request → Sent back to customers.  
- **Additional Features:**  
  - **Redis** (Data storage)  
  - **KeyVault** (Stores security keys)  
  - **NewRelic** (App monitoring, being replaced by **Vector**)  

### **6. Data Pipelines**
#### **Pipeline Architecture**
- Data ingestion from **New Relic** & **Splunk**.  
- **Tardigrade Pipelines:** Processes data and extracts features like product, store, weather.  
- **Feature Storage:** Redis cache via **upload_to_redis pipeline**.  
- **Model Training:**  
  - Models trained using **DDQN & DICF**.  
  - Stored in **mlflow** using **model_train_and_promote pipeline**.  
- **Inference:** Model predictions used in DDT application → Customer recommendations.  

#### **Streaming Data Processing**
- **Product Streaming:** 24/7 store inventory monitoring → Updates Redis cache.  
- **Scala-based Streaming Jobs.**  

### **7. Main Data Pipelines**
1. **Splunk Pipeline:**  
   - Splunk_hourly (Runs every hour, auto backfills)  
   - Splunk_weekly (Runs weekly, backfills missing data)  
2. **Tardigrade Pipeline:** Generates product, store, weather features.  
3. **Weekly Measurement:** Generates **previous Sunday data**, uploads to Redis.  
4. **Weekly Product Insights:** Provides insights for dashboards.  

### **8. Datadog Alerts & Monitoring**
- If issues arise, **Datadog logs** are checked for errors.  
- Issues analyzed based on logs and resolved accordingly.  

This structured information is included in the PPT. Let me know if you need changes!
